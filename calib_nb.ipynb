{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4e7ac5",
   "metadata": {},
   "source": [
    "# Notebook: run calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917ac2b",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import standard Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# We also import our own packages\n",
    "import inputs.data as inpdt\n",
    "import inputs.parameters_and_options as inpprm\n",
    "import equilibrium.compute_equilibrium as eqcmp\n",
    "import equilibrium.functions_dynamic as eqdyn\n",
    "import calibration.calib_main_func as calmain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2aff6c",
   "metadata": {},
   "source": [
    "### Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46bd18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "path_code = '..'\n",
    "path_folder = path_code + '/2. Data/'\n",
    "path_precalc_inp = path_folder + '0. Precalculated inputs/'\n",
    "path_data = path_folder + 'data_Cape_Town/'\n",
    "path_precalc_transp = path_folder + 'precalculated_transport/'\n",
    "path_scenarios = path_folder + 'data_Cape_Town/Scenarios/'\n",
    "path_outputs = path_code + '/4. Sorties/'\n",
    "path_floods = path_folder + \"FATHOM/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78ce01",
   "metadata": {},
   "source": [
    "## Import parameters and options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f807fa",
   "metadata": {},
   "source": [
    "### We import default parameter and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = inpprm.import_options()\n",
    "param = inpprm.import_param(\n",
    "    path_precalc_inp, path_outputs, path_folder, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba77ebe",
   "metadata": {},
   "source": [
    "### We also set custom options for this simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c49943",
   "metadata": {},
   "source": [
    "#### We first set options regarding structural assumptions used in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942829c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy for taking floods into account in agents' choices\n",
    "options[\"agents_anticipate_floods\"] = 1\n",
    "# Dummy for preventing new informal settlement development\n",
    "options[\"informal_land_constrained\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbf26e",
   "metadata": {},
   "source": [
    "#### Then we set options regarding flood data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a94c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy for taking pluvial floods into account (on top of fluvial floods)\n",
    "options[\"pluvial\"] = 1\n",
    "# Dummy for reducing pluvial risk for (better protected) formal structures\n",
    "options[\"correct_pluvial\"] = 1\n",
    "# Dummy for taking coastal floods into account (on top of fluvial floods)\n",
    "options[\"coastal\"] = 1\n",
    "# Digital elevation model to be used with coastal floods (MERITDEM or NASADEM)\n",
    "# NB: MERITDEM is also the DEM used for fluvial and pluvial flood data\n",
    "options[\"dem\"] = \"MERITDEM\"\n",
    "# Dummy for taking defended (vs. undefended) fluvial flood maps\n",
    "# NB: FATHOM recommends to use undefended maps due to the high uncertainty\n",
    "# in infrastructure modelling\n",
    "options[\"defended\"] = 1\n",
    "# Dummy for taking sea-level rise into account in coastal flood data\n",
    "# NB: Projections are up to 2050, based upon IPCC AR5 assessment for the\n",
    "# RCP 8.5 scenario\n",
    "options[\"slr\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846062b",
   "metadata": {},
   "source": [
    "#### We also set options for scenarios on time-moving exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Must be set to 1/2/3 for low/medium/high growth scenario\n",
    "options[\"inc_ineq_scenario\"] = 2\n",
    "options[\"pop_growth_scenario\"] = 3\n",
    "options[\"fuel_price_scenario\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df177",
   "metadata": {},
   "source": [
    "#### Finally, we set options regarding data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d389b5e",
   "metadata": {},
   "source": [
    "Default is set at zero to save computing time\n",
    "(data is simply loaded in the model)\n",
    "\n",
    "NB: this is only needed to create the data for the first time, or when the\n",
    "source is changed, so that pre-processed data is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c7e07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Dummy for converting small-area-level (SAL) data into grid-level data\n",
    "# (used for result validation)\n",
    "options[\"convert_sal_data\"] = 0\n",
    "# Dummy for computing expected income net of commuting costs on the basis\n",
    "# of calibrated wages\n",
    "options[\"compute_net_income\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53e9bbd",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e4722",
   "metadata": {},
   "source": [
    "### Basic geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db03e1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "grid, center = inpdt.import_grid(path_data)\n",
    "amenities = inpdt.import_amenities(path_precalc_inp, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe12098",
   "metadata": {},
   "source": [
    "### Macro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90772654",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "(interest_rate, population, housing_type_data, total_RDP\n",
    " ) = inpdt.import_macro_data(param, path_scenarios, path_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a6f9f",
   "metadata": {},
   "source": [
    "### Households and income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9772cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_class_by_housing_type = inpdt.import_hypothesis_housing_type()\n",
    "\n",
    "(mean_income, households_per_income_class, average_income, income_mult,\n",
    " income_2011, households_per_income_and_housing\n",
    " ) = inpdt.import_income_classes_data(param, path_data)\n",
    "\n",
    "# NB: we create this parameter to maintain money illusion in simulations\n",
    "# (see eqsim.run_simulation function)\n",
    "param[\"income_year_reference\"] = mean_income\n",
    "\n",
    "# Other data at SP (small place) level used for calibration and validation\n",
    "(data_rdp, housing_types_sp, data_sp, mitchells_plain_grid_2011,\n",
    " grid_formal_density_HFA, threshold_income_distribution, income_distribution,\n",
    " cape_town_limits) = inpdt.import_households_data(path_precalc_inp)\n",
    "\n",
    "# Import nb of households per pixel, by housing type (from SAL data)\n",
    "# NB: RDP housing is included in formal, and there are both formal and informal\n",
    "# backyards\n",
    "if options[\"convert_sal_data\"] == 1:\n",
    "    housing_types = inpdt.import_sal_data(grid, path_folder, path_data,\n",
    "                                          housing_type_data)\n",
    "housing_types = pd.read_excel(path_folder + 'housing_types_grid_sal.xlsx')\n",
    "housing_types[np.isnan(housing_types)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdac1b9",
   "metadata": {},
   "source": [
    "### Land use projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import basic projections\n",
    "(spline_RDP, spline_estimate_RDP, spline_land_RDP,\n",
    " spline_land_backyard, spline_land_informal, spline_land_constraints,\n",
    " number_properties_RDP) = (\n",
    "     inpdt.import_land_use(grid, options, param, data_rdp, housing_types,\n",
    "                           housing_type_data, path_data, path_folder)\n",
    "     )\n",
    "\n",
    "# We correct areas for each housing type at baseline year for the amount of\n",
    "# constructible land in each type\n",
    "coeff_land = inpdt.import_coeff_land(\n",
    "    spline_land_constraints, spline_land_backyard, spline_land_informal,\n",
    "    spline_land_RDP, param, 0)\n",
    "\n",
    "# We import housing heigth limits\n",
    "housing_limit = inpdt.import_housing_limit(grid, param)\n",
    "\n",
    "# We update parameter vector with construction parameters\n",
    "# (relies on loaded data) and compute other variables\n",
    "(param, minimum_housing_supply, agricultural_rent\n",
    " ) = inpprm.import_construction_parameters(\n",
    "    param, grid, housing_types_sp, data_sp[\"dwelling_size\"],\n",
    "    mitchells_plain_grid_2011, grid_formal_density_HFA, coeff_land,\n",
    "    interest_rate, options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa35ad1",
   "metadata": {},
   "source": [
    "### Import flood data (takes some time when agents anticipate floods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a09b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If agents anticipate floods, we return output from damage functions\n",
    "if options[\"agents_anticipate_floods\"] == 1:\n",
    "    (fraction_capital_destroyed, structural_damages_small_houses,\n",
    "     structural_damages_medium_houses, structural_damages_large_houses,\n",
    "     content_damages, structural_damages_type1, structural_damages_type2,\n",
    "     structural_damages_type3a, structural_damages_type3b,\n",
    "     structural_damages_type4a, structural_damages_type4b\n",
    "     ) = inpdt.import_full_floods_data(options, param, path_folder,\n",
    "                                       housing_type_data)\n",
    "\n",
    "# Else, we set those outputs as zero\n",
    "# NB: 24014 is the number of grid pixels\n",
    "elif options[\"agents_anticipate_floods\"] == 0:\n",
    "    fraction_capital_destroyed = pd.DataFrame()\n",
    "    fraction_capital_destroyed[\"structure_formal_2\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_formal_1\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_subsidized_2\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_subsidized_1\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"contents_formal\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"contents_informal\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"contents_subsidized\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"contents_backyard\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_backyards\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_formal_backyards\"] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_informal_backyards\"\n",
    "                               ] = np.zeros(24014)\n",
    "    fraction_capital_destroyed[\"structure_informal_settlements\"\n",
    "                               ] = np.zeros(24014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2bb74c",
   "metadata": {},
   "source": [
    "### Import scenarios (for time-moving variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spline_agricultural_rent, spline_interest_rate,\n",
    " spline_population_income_distribution, spline_inflation,\n",
    " spline_income_distribution, spline_population,\n",
    " spline_income, spline_minimum_housing_supply, spline_fuel\n",
    " ) = eqdyn.import_scenarios(income_2011, param, grid, path_scenarios,\n",
    "                            options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92711f73",
   "metadata": {},
   "source": [
    "### Import income net of commuting costs (for all time periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52f7c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if options[\"compute_net_income\"] == 1:\n",
    "    (incomeNetOfCommuting, modalShares, ODflows, averageIncome\n",
    "     ) = inpdt.import_transport_data(\n",
    "         grid, param, 0, households_per_income_class, average_income,\n",
    "         spline_inflation, spline_fuel,\n",
    "         spline_population_income_distribution, spline_income_distribution,\n",
    "         path_precalc_inp, path_precalc_transp, 'GRID', options)\n",
    "\n",
    "income_net_of_commuting_costs = np.load(\n",
    "    path_precalc_transp + 'GRID_incomeNetOfCommuting_0.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6dab1",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92844947",
   "metadata": {},
   "source": [
    "### Define dominant income group in each census block (SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0657006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In one case, we base our definition on the median income at the SP-level:\n",
    "# we consider as dominant the group corresponding to the highest income\n",
    "# threshold is crossed\n",
    "if options[\"correct_dominant_incgrp\"] == 0:\n",
    "    data_income_group = np.zeros(len(data_sp[\"income\"]))\n",
    "    for j in range(0, param[\"nb_of_income_classes\"] - 1):\n",
    "        data_income_group[data_sp[\"income\"] >\n",
    "                          threshold_income_distribution[j]] = j+1\n",
    "\n",
    "# In the other case, we just consider the most numerous income group in each\n",
    "# census block\n",
    "elif options[\"correct_dominant_incgrp\"] == 1:\n",
    "    data_income_group = np.zeros(len(income_distribution))\n",
    "    for i in range(0, len(income_distribution)):\n",
    "        data_income_group[i] = np.argmax(income_distribution[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fb0af",
   "metadata": {},
   "source": [
    "Although the second option seems more logical, we take the first one as\n",
    "default since we are going to consider median SP prices, and we want\n",
    "associated net income to be in line with those values to avoid a sample\n",
    "selection bias in our regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256f411",
   "metadata": {},
   "source": [
    "### Obtain number of formal private housing units per SP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180292f8",
   "metadata": {},
   "source": [
    "NB: it is not clear whether RDP are included in SP formal count, and\n",
    "if they should be taken out based on imperfect cadastral estimations.\n",
    "For reference, we include the two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23489196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if options[\"substract_RDP_from_formal\"] == 1:\n",
    "    # We retrieve number of RDP units per SP from grid-level data\n",
    "    grid_intersect = pd.read_csv(path_data + 'grid_SP_intersect.csv',\n",
    "                                 sep=';')\n",
    "    # When pixels are associated to several SPs, we allocate them to the\n",
    "    # one with the biggest intersection area.\n",
    "    # NB: it would be more rigorous to split the number of RDP across\n",
    "    # SPs according to their respective intersection areas, but this is\n",
    "    # unlikely to change much\n",
    "    grid_intersect = grid_intersect.groupby('ID_grille').max('Area')\n",
    "    data_rdp[\"ID_grille\"] = data_rdp.index\n",
    "    data_rdp[\"ID_grille\"] = data_rdp[\"ID_grille\"] + 1\n",
    "\n",
    "    rdp_grid = pd.merge(data_rdp, grid_intersect, on=\"ID_grille\",\n",
    "                        how=\"outer\")\n",
    "    rdp_sp = rdp_grid.groupby('SP_CODE')['count'].sum()\n",
    "    rdp_sp = rdp_sp.reset_index()\n",
    "    rdp_sp = rdp_sp.rename(columns={'SP_CODE': 'sp_code'})\n",
    "    # We just fill the list with unmatched SPs to get the full SP vector\n",
    "    rdp_sp_fill = pd.merge(rdp_sp, data_sp['sp_code'], on=\"sp_code\",\n",
    "                           how=\"outer\")\n",
    "    rdp_sp_fill['count'] = rdp_sp_fill['count'].fillna(0)\n",
    "    rdp_sp_fill = rdp_sp_fill.sort_values(by='sp_code')\n",
    "\n",
    "    data_number_formal = (\n",
    "        housing_types_sp.total_dwellings_SP_2011\n",
    "        - housing_types_sp.backyard_SP_2011\n",
    "        - housing_types_sp.informal_SP_2011\n",
    "        - rdp_sp_fill['count'])\n",
    "\n",
    "elif options[\"substract_RDP_from_formal\"] == 0:\n",
    "    data_number_formal = (\n",
    "        housing_types_sp.total_dwellings_SP_2011\n",
    "        - housing_types_sp.backyard_SP_2011\n",
    "        - housing_types_sp.informal_SP_2011\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e4c2f",
   "metadata": {},
   "source": [
    "Given the uncertainty surrounding RDP counts, we take the second option as\n",
    "default and prefer to rely on sample selection (see below) to exclude the SPs\n",
    "where RDP housing is likely to drive most of our results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7dfb20",
   "metadata": {},
   "source": [
    "### Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ef95",
   "metadata": {},
   "source": [
    "As the relations we are going to estimate are only true for the\n",
    "formal private sector, we exclude SPs in the bottom quintile of property\n",
    "prices and for which more than 5% of households are reported to live in\n",
    "informal housing (settlements + backyards). We also exclude \"rural\" SPs\n",
    "(i.e., those that are large, with a small share than can be urbanized)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b39aa",
   "metadata": {},
   "source": [
    "We also add options to consider other criteria, namely we offer to\n",
    "exclude poorest income group (which is in effect crowded out from the\n",
    "formal sector), as well as Mitchell's Plain (as its housing market is\n",
    "very specific) and far-away land (for which we have few observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if options[\"correct_selected_density\"] == 0:\n",
    "    selected_density = (\n",
    "        (data_sp[\"price\"] > np.nanquantile(data_sp[\"price\"], 0.2))\n",
    "        & (data_number_formal\n",
    "           > 0.95 * housing_types_sp.total_dwellings_SP_2011)\n",
    "        & (data_sp[\"unconstrained_area\"]\n",
    "            < np.nanquantile(data_sp[\"unconstrained_area\"], 0.8))\n",
    "        & (data_sp[\"unconstrained_area\"] > 0.6 * 1000000 * data_sp[\"area\"])\n",
    "        )\n",
    "\n",
    "elif (options[\"correct_selected_density\"] == 1\n",
    "      and options[\"correct_mitchells_plain\"] == 0):\n",
    "    selected_density = (\n",
    "        (data_sp[\"price\"] > np.nanquantile(data_sp[\"price\"], 0.2))\n",
    "        & (data_number_formal\n",
    "           > 0.95 * housing_types_sp.total_dwellings_SP_2011)\n",
    "        & (data_sp[\"unconstrained_area\"]\n",
    "            < np.nanquantile(data_sp[\"unconstrained_area\"], 0.8))\n",
    "        & (data_sp[\"unconstrained_area\"] > 0.6 * 1000000 * data_sp[\"area\"])\n",
    "        & (data_income_group > 0)\n",
    "        & (data_sp[\"distance\"] < 40)\n",
    "        )\n",
    "\n",
    "elif (options[\"correct_selected_density\"] == 1\n",
    "      and options[\"correct_mitchells_plain\"] == 1):\n",
    "    selected_density = (\n",
    "        (data_sp[\"price\"] > np.nanquantile(data_sp[\"price\"], 0.2))\n",
    "        & (data_number_formal\n",
    "           > 0.95 * housing_types_sp.total_dwellings_SP_2011)\n",
    "        & (data_sp[\"unconstrained_area\"]\n",
    "            < np.nanquantile(data_sp[\"unconstrained_area\"], 0.8))\n",
    "        & (data_sp[\"unconstrained_area\"] > 0.6 * 1000000 * data_sp[\"area\"])\n",
    "        & (data_income_group > 0)\n",
    "        & (data_sp[\"mitchells_plain\"] == 0)\n",
    "        & (data_sp[\"distance\"] < 40)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d1a46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We pick the second choice as our default since it is more conservative than\n",
    "the first, and less ad hoc than the third one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6911c3",
   "metadata": {},
   "source": [
    "## Calibrate construction function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce64609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We estimate construction function parameters based on a log-linearization\n",
    "# of the housing market clearing condition\n",
    "coeff_b, coeff_a, coeffKappa = calmain.estim_construct_func_param(\n",
    "    options, param, data_sp, threshold_income_distribution,\n",
    "    income_distribution, data_rdp, housing_types_sp,\n",
    "    data_number_formal, data_income_group, selected_density,\n",
    "    path_data, path_precalc_inp, path_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3cfaa",
   "metadata": {},
   "source": [
    "NB: The results are automatically saved for later use in simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce229de1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We update parameter vector\n",
    "param[\"coeff_a\"] = coeff_a\n",
    "param[\"coeff_b\"] = coeff_b\n",
    "param[\"coeff_A\"] = coeffKappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c991c3",
   "metadata": {},
   "source": [
    "## Calibrate incomes and gravity parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d8fd3",
   "metadata": {},
   "source": [
    "We scan values for the gravity parameter to estimate incomes as a\n",
    "function of it. The value range is set by trial and error: the wider the\n",
    "range you want to test, the longer. In principle, we should find a value\n",
    "within a coarse interval before going to the finer level: this may require\n",
    "several iterations if the underlying data changes.\n",
    "NB: we do that as it is too long and complex to run a solver directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd421c5",
   "metadata": {},
   "source": [
    "Then, we select the income-gravity pair that best fits the distribution\n",
    "of commuters over distance from the CBD.\n",
    "NB: we need to proceed in twos steps as there is no separate identification\n",
    "of the gravity parameter and the net incomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by selecting the range over which we want to scan\n",
    "if options[\"scan_type\"] == \"rough\":\n",
    "    list_lambda = 10 ** np.arange(0.40, 0.51, 0.05)\n",
    "if options[\"scan_type\"] == \"normal\":\n",
    "    list_lambda = 10 ** np.arange(0.42, 0.441, 0.01)\n",
    "if options[\"scan_type\"] == \"fine\":\n",
    "    list_lambda = 10 ** np.arange(0.427, 0.4291, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then run the function that returns the calibrated outputs\n",
    "(incomeCentersKeep, lambdaKeep, cal_avg_income, scoreKeep,\n",
    " bhattacharyyaDistances) = (\n",
    "    calmain.estim_incomes_and_gravity(\n",
    "        param, grid, list_lambda, households_per_income_class,\n",
    "        average_income, income_distribution, spline_inflation, spline_fuel,\n",
    "        spline_population_income_distribution, spline_income_distribution,\n",
    "        path_data, path_precalc_inp, path_precalc_transp, options)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6c27f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We update the parameter vector\n",
    "param[\"lambda\"] = np.array(lambdaKeep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004a410",
   "metadata": {},
   "source": [
    "## Calibrate utility function parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18088efb",
   "metadata": {},
   "source": [
    "We compute local incomes net of commuting costs at the SP (not grid)\n",
    "level that is used in calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that lambda and calibrated incomes have an impact here: from now on,\n",
    "# we will stop loading precalibrated parameters to rely on the newly\n",
    "# calibrated parameters that we just saved\n",
    "options[\"load_precal_param\"] = 0\n",
    "\n",
    "(incomeNetOfCommuting, *_\n",
    " ) = inpdt.import_transport_data(\n",
    "     grid, param, 0, households_per_income_class, average_income,\n",
    "     spline_inflation, spline_fuel,\n",
    "     spline_population_income_distribution, spline_income_distribution,\n",
    "     path_precalc_inp, path_precalc_transp, 'SP', options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b338b0b",
   "metadata": {},
   "source": [
    "Then we calibrate utility function parameters based on the maximization\n",
    "of a composite likelihood that reproduces the fit on exogenous amenities,\n",
    "dwelling sizes, and income sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Here, we also have an impact from construction parameters and sample\n",
    "# selection (+ number of formal units)\n",
    "(calibratedUtility_beta, calibratedUtility_q0, cal_amenities\n",
    " ) = calmain.estim_util_func_param(\n",
    "     data_number_formal, data_income_group, housing_types_sp, data_sp,\n",
    "     coeff_a, coeff_b, coeffKappa, interest_rate,\n",
    "     incomeNetOfCommuting, selected_density, path_data, path_precalc_inp,\n",
    "     options, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0aeeb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We update parameter vector\n",
    "param[\"beta\"] = calibratedUtility_beta\n",
    "param[\"q0\"] = calibratedUtility_q0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5801da",
   "metadata": {},
   "source": [
    "## Calibrate disamenity index for informal backyards + settlements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93312685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to recompute income net of commuting costs at baseline\n",
    "# year since calibrated income has changed\n",
    "(incomeNetOfCommuting, modalShares, ODflows, averageIncome\n",
    " ) = inpdt.import_transport_data(\n",
    "     grid, param, 0, households_per_income_class, average_income,\n",
    "     spline_inflation, spline_fuel,\n",
    "     spline_population_income_distribution, spline_income_distribution,\n",
    "     path_precalc_inp, path_precalc_transp, 'GRID', options)\n",
    "\n",
    "income_net_of_commuting_costs = np.load(\n",
    "    path_precalc_transp + 'GRID_incomeNetOfCommuting_0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46beab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we do the same for the amenity index\n",
    "amenities = inpdt.import_amenities(path_precalc_inp, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0f7c",
   "metadata": {},
   "source": [
    "NB: Since disamenity index calibration relies on the model fit and is not\n",
    "computed a priori (contrary to other parameters), the options set in the\n",
    "preamble should be the same as the ones used in the main script, so that\n",
    "the calibrated values are in line with the structural assumptions used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3844a0f",
   "metadata": {},
   "source": [
    "### We start with a general (not location-specific) calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c45447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a range of disamenity values which we would like to scan,\n",
    "# and arrange them in a grid\n",
    "list_amenity_backyard = np.arange(0.64, 0.681, 0.01)\n",
    "list_amenity_settlement = np.arange(0.60, 0.641, 0.01)\n",
    "housing_type_total = pd.DataFrame(np.array(np.meshgrid(\n",
    "    list_amenity_backyard, list_amenity_settlement)).T.reshape(-1, 2))\n",
    "housing_type_total.columns = [\"param_backyard\", \"param_settlement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a06dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize output vector\n",
    "housing_type_total[\"formal\"] = np.zeros(\n",
    "    len(housing_type_total.param_backyard))\n",
    "housing_type_total[\"backyard\"] = np.zeros(\n",
    "    len(housing_type_total.param_backyard))\n",
    "housing_type_total[\"informal\"] = np.zeros(\n",
    "    len(housing_type_total.param_backyard))\n",
    "housing_type_total[\"subsidized\"] = np.zeros(\n",
    "    len(housing_type_total.param_backyard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the number of total iterations (to have an intuition of how long\n",
    "# the process will take)\n",
    "number_total_iterations = (\n",
    "    len(list_amenity_backyard) * len(list_amenity_settlement))\n",
    "print(f\"** Calibration: {number_total_iterations} iterations **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bd028",
   "metadata": {},
   "source": [
    "We are going to compute the initial state equilibrium for each pair of\n",
    "parameters, and retain the one that best fits the observed number of\n",
    "households in informal settlements + backyards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0dac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(list_amenity_backyard)):\n",
    "    for j in range(0, len(list_amenity_settlement)):\n",
    "\n",
    "        # We set input values\n",
    "        param[\"amenity_backyard\"] = list_amenity_backyard[i]\n",
    "        param[\"amenity_settlement\"] = list_amenity_settlement[j]\n",
    "        param[\"pockets\"] = np.ones(24014) * param[\"amenity_settlement\"]\n",
    "        param[\"backyard_pockets\"] = (np.ones(24014)\n",
    "                                     * param[\"amenity_backyard\"])\n",
    "\n",
    "        # We run the algorithm\n",
    "        (initial_state_utility,\n",
    "         initial_state_error,\n",
    "         initial_state_simulated_jobs,\n",
    "         initial_state_households_housing_types,\n",
    "         initial_state_household_centers,\n",
    "         initial_state_households,\n",
    "         initial_state_dwelling_size,\n",
    "         initial_state_housing_supply,\n",
    "         initial_state_rent,\n",
    "         initial_state_rent_matrix,\n",
    "         initial_state_capital_land,\n",
    "         initial_state_average_income,\n",
    "         initial_state_limit_city) = eqcmp.compute_equilibrium(\n",
    "             fraction_capital_destroyed,\n",
    "             amenities,\n",
    "             param,\n",
    "             housing_limit,\n",
    "             population,\n",
    "             households_per_income_class,\n",
    "             total_RDP,\n",
    "             coeff_land,\n",
    "             income_net_of_commuting_costs,\n",
    "             grid,\n",
    "             options,\n",
    "             agricultural_rent,\n",
    "             interest_rate,\n",
    "             number_properties_RDP,\n",
    "             average_income,\n",
    "             mean_income,\n",
    "             income_class_by_housing_type,\n",
    "             minimum_housing_supply,\n",
    "             param[\"coeff_A\"],\n",
    "             income_2011)\n",
    "\n",
    "        # We fill output matrix with the total number of HHs per housing\n",
    "        # type for given values of backyard and informal amenity parameters\n",
    "        housing_type_total.iloc[\n",
    "            (housing_type_total.param_backyard\n",
    "             == param[\"amenity_backyard\"])\n",
    "            & (housing_type_total.param_settlement\n",
    "               == param[\"amenity_settlement\"]),\n",
    "            2:6] = np.nansum(initial_state_households_housing_types, 1)\n",
    "\n",
    "        # We update the iteration count and print progress made\n",
    "        iteration_number = i * len(list_amenity_settlement) + j + 1\n",
    "        print(f\"iteration {iteration_number}/{number_total_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the error between simulated and observed number of households\n",
    "# in each housing type (without RDP, which is exogenously set equal to data)\n",
    "distance_share = np.abs(\n",
    "    housing_type_total.iloc[:, 2:5] - housing_type_data[None, 0:3])\n",
    "\n",
    "# We define the score that we want to minimize as the sum of the errors for\n",
    "# informal backyards and informal settlements\n",
    "distance_share_score = (\n",
    "    distance_share.iloc[:, 1] + distance_share.iloc[:, 2])\n",
    "\n",
    "# We select the arguments associated with the minimum\n",
    "which = np.argmin(distance_share_score)\n",
    "min_score = np.nanmin(distance_share_score)\n",
    "calibrated_amenities = housing_type_total.iloc[which, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87792fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We update parameter vector\n",
    "param[\"amenity_backyard\"] = calibrated_amenities[0]\n",
    "param[\"amenity_settlement\"] = calibrated_amenities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the output directory and save the values\n",
    "try:\n",
    "    os.mkdir(path_precalc_inp)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "np.save(path_precalc_inp + 'param_amenity_backyard.npy',\n",
    "        param[\"amenity_backyard\"])\n",
    "np.save(path_precalc_inp + 'param_amenity_settlement.npy',\n",
    "        param[\"amenity_settlement\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444279d",
   "metadata": {},
   "source": [
    "### Calibrate location-specific disamenity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64847dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is set to 1 but can be changed if we fear overfit of the model\n",
    "if options[\"location_based_calib\"] == 1:\n",
    "\n",
    "    # We start from where we left (to gain time) and compute the\n",
    "    # equilibrium again\n",
    "\n",
    "    # We first initialize input values\n",
    "\n",
    "    index = 0\n",
    "    index_max = 50\n",
    "    metrics = np.zeros(index_max)\n",
    "\n",
    "    param[\"pockets\"] = np.zeros(24014) + param[\"amenity_settlement\"]\n",
    "    save_param_informal_settlements = np.zeros((index_max, 24014))\n",
    "    metrics_is = np.zeros(index_max)\n",
    "    param[\"backyard_pockets\"] = np.zeros(24014) + param[\"amenity_backyard\"]\n",
    "    save_param_backyards = np.zeros((index_max, 24014))\n",
    "    metrics_ib = np.zeros(index_max)\n",
    "\n",
    "    # We run the algorithm\n",
    "    (initial_state_utility,\n",
    "     initial_state_error,\n",
    "     initial_state_simulated_jobs,\n",
    "     initial_state_households_housing_types,\n",
    "     initial_state_household_centers,\n",
    "     initial_state_households,\n",
    "     initial_state_dwelling_size,\n",
    "     initial_state_housing_supply,\n",
    "     initial_state_rent,\n",
    "     initial_state_rent_matrix,\n",
    "     initial_state_capital_land,\n",
    "     initial_state_average_income,\n",
    "     initial_state_limit_city\n",
    "     ) = eqcmp.compute_equilibrium(\n",
    "         fraction_capital_destroyed,\n",
    "         amenities,\n",
    "         param,\n",
    "         housing_limit,\n",
    "         population,\n",
    "         households_per_income_class,\n",
    "         total_RDP,\n",
    "         coeff_land,\n",
    "         income_net_of_commuting_costs,\n",
    "         grid,\n",
    "         options,\n",
    "         agricultural_rent,\n",
    "         interest_rate,\n",
    "         number_properties_RDP,\n",
    "         average_income,\n",
    "         mean_income,\n",
    "         income_class_by_housing_type,\n",
    "         minimum_housing_supply,\n",
    "         param[\"coeff_A\"],\n",
    "         income_2011)\n",
    "\n",
    "    # We set the maximum number of iterations\n",
    "    number_total_iterations = index_max\n",
    "\n",
    "    # Then we optimize over the number of households per housing type\n",
    "    # PER PIXEL, and not just on the aggregate number (to acccount for\n",
    "    # differing disamenities per location, e.g. eviction probability,\n",
    "    # infrastructure networks, etc.)\n",
    "\n",
    "    # To do so, we use granular housing_types variable (from SAL data) instead\n",
    "    # of aggregate housing_types variable\n",
    "\n",
    "    for index in range(0, index_max):\n",
    "\n",
    "        # INFORMAL SETTLEMENTS\n",
    "\n",
    "        # We initialize output vector\n",
    "        diff_is = np.zeros(24014)\n",
    "        for i in range(0, 24014):\n",
    "            # We store the error term\n",
    "            diff_is[i] = (housing_types.informal_grid[i]\n",
    "                          - initial_state_households_housing_types[2, :][i]\n",
    "                          )\n",
    "            # We apply an empirical reweighting that helps convergence\n",
    "            adj = (diff_is[i] / 150000)\n",
    "            # We increase the amenity score when we underestimate the nb of\n",
    "            # households\n",
    "            param[\"pockets\"][i] = param[\"pockets\"][i] + adj\n",
    "        # We store iteration outcome and prevent extreme sorting from\n",
    "        # happening due to the amenity score\n",
    "        metrics_is[index] = sum(np.abs(diff_is))\n",
    "        param[\"pockets\"][param[\"pockets\"] < 0.05] = 0.05\n",
    "        param[\"pockets\"][param[\"pockets\"] > 0.99] = 0.99\n",
    "        save_param_informal_settlements[index, :] = param[\"pockets\"]\n",
    "\n",
    "        # INFORMAL BACKYARDS\n",
    "\n",
    "        # We initialize output vector\n",
    "        diff_ib = np.zeros(24014)\n",
    "        for i in range(0, 24014):\n",
    "            # Note that we add an option depending on whether we restrict\n",
    "            # ourselves to informal backyards (default) or all kinds of\n",
    "            # backyards (not warranted given the standardized structure\n",
    "            # assumed in the model)\n",
    "            if options[\"actual_backyards\"] == 1:\n",
    "                diff_ib[i] = (\n",
    "                    housing_types.backyard_informal_grid[i]\n",
    "                    + housing_types.backyard_formal_grid[i]\n",
    "                    - initial_state_households_housing_types[1, :][i])\n",
    "            elif options[\"actual_backyards\"] == 0:\n",
    "                diff_ib[i] = (\n",
    "                    housing_types.backyard_informal_grid[i]\n",
    "                    - initial_state_households_housing_types[1, :][i])\n",
    "            # We help convergence and update parameter\n",
    "            adj = (diff_ib[i] / 75000)\n",
    "            param[\"backyard_pockets\"][i] = (\n",
    "                param[\"backyard_pockets\"][i] + adj)\n",
    "        # We store iteration output and prevent extreme sorting\n",
    "        metrics_ib[index] = sum(np.abs(diff_ib))\n",
    "        param[\"backyard_pockets\"][param[\"backyard_pockets\"] < 0.05] = 0.05\n",
    "        param[\"backyard_pockets\"][param[\"backyard_pockets\"] > 0.99] = 0.99\n",
    "        save_param_backyards[index, :] = param[\"backyard_pockets\"]\n",
    "\n",
    "        # We retain the sum of the errors as our minimization objective\n",
    "        metrics[index] = metrics_is[index] + metrics_ib[index]\n",
    "\n",
    "        # We run the equilibrium again with updated values of\n",
    "        # informal/backyard housing disamenity indices, then go to the next\n",
    "        # iteration\n",
    "\n",
    "        (initial_state_utility, initial_state_error,\n",
    "         initial_state_simulated_jobs,\n",
    "         initial_state_households_housing_types,\n",
    "         initial_state_household_centers,\n",
    "         initial_state_households, initial_state_dwelling_size,\n",
    "         initial_state_housing_supply, initial_state_rent,\n",
    "         initial_state_rent_matrix, initial_state_capital_land,\n",
    "         initial_state_average_income, initial_state_limit_city\n",
    "         ) = eqcmp.compute_equilibrium(\n",
    "             fraction_capital_destroyed, amenities, param, housing_limit,\n",
    "             population, households_per_income_class, total_RDP,\n",
    "             coeff_land, income_net_of_commuting_costs, grid, options,\n",
    "             agricultural_rent, interest_rate, number_properties_RDP,\n",
    "             average_income, mean_income, income_class_by_housing_type,\n",
    "             minimum_housing_supply, param[\"coeff_A\"], income_2011)\n",
    "\n",
    "        iteration_number = index + 1\n",
    "\n",
    "        print(f\"iteration {iteration_number}/{number_total_iterations}\")\n",
    "\n",
    "    # We pick the set of parameters that minimize the sum of absolute diffs\n",
    "    # between data and simulation\n",
    "    score_min = np.min(metrics)\n",
    "    index_min = np.argmin(metrics)\n",
    "\n",
    "    # We update the parameter vector\n",
    "    param[\"pockets\"] = save_param_informal_settlements[index_min]\n",
    "    param[\"backyard_pockets\"] = save_param_backyards[index_min]\n",
    "\n",
    "    # We print the basic distribution of the calibrated parameters\n",
    "    print(np.nanmin(param[\"pockets\"]))\n",
    "    print(np.nanmean(param[\"pockets\"]))\n",
    "    print(np.nanmax(param[\"pockets\"]))\n",
    "    print(np.nanmin(param[\"backyard_pockets\"]))\n",
    "    print(np.nanmean(param[\"backyard_pockets\"]))\n",
    "    print(np.nanmax(param[\"backyard_pockets\"]))\n",
    "\n",
    "    # We create the output directory and save values\n",
    "    try:\n",
    "        os.mkdir(path_precalc_inp)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    np.save(path_precalc_inp + 'param_pockets.npy',\n",
    "            param[\"pockets\"])\n",
    "    np.save(path_precalc_inp + 'param_backyards.npy',\n",
    "            param[\"backyard_pockets\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "region,endregion",
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
