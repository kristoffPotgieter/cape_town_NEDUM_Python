{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b809a35c-62b1-4fef-b69d-a00417343a94",
   "metadata": {},
   "source": [
    "## Accessing Deltares global flood data on Azure\n",
    "\n",
    "[Deltares](https://www.deltares.nl/en/) has produced a series of global inundation maps of flood depth using a geographic information systems (GIS)-based inundation model that takes into account water level attenuation and is forced by sea level. Multiple datasets were created using various digital elevation models (DEMs) at multiple resolutions under two different sea level rise (SLR) conditions: current (2018) and 2050. \n",
    "\n",
    "This notebook provides an example of accessing global flood data from blob storage on Azure.\n",
    "\n",
    "This dataset is stored in the West Europe Azure region, so this notebook will run most efficiently on Azure compute located in the same region. If you are using this data for environmental science applications, consider applying for an AI for Earth grant to support your compute requirements.\n",
    "\n",
    "Complete documentation for this dataset is available at https://aka.ms/ai4edata-deltares-gfm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215a7ef-bdd5-497e-b9bb-53621ef8169c",
   "metadata": {},
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30185f5a-50e3-4c9d-8644-1cb8b48aadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Not used directly, but needs to be installed to read NetCDF files with xarray\n",
    "import h5py\n",
    "import h5netcdf\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "from shapely.geometry import shape\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bab087",
   "metadata": {},
   "source": [
    "### Define an area of interest\n",
    "\n",
    "The data is 90m (3'') at a global scale, but most relevant in coastal areas. Let's zoom in a on a flood-prone region of South Africa (City of Cape Town) by defining a bounding box and clipping our xarray dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359447d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coct_geojson = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-33.459480262765084, 18.44984306000178],\n",
    "            [-34.0349865125429, 18.287794733823475],\n",
    "            [-34.36664252903477, 18.491041786996263],\n",
    "            [-34.075946082570105, 19.012892328926387]\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "poly = shape(coct_geojson)\n",
    "miny, minx, maxy, maxx = poly.bounds\n",
    "print(\"AoI bounds:\", poly.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13218229-f4ec-4d8f-8915-112ddcd6df4c",
   "metadata": {},
   "source": [
    "### Create a local Dask cluster\n",
    "\n",
    "Enable parallel reads and processing of data using Dask and xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2328949-b3dc-4dc9-abfa-c643ec7fbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False)\n",
    "print(f\"/proxy/{client.scheduler_info()['services']['dashboard']}/status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a2ac9-2281-42d7-b289-d17c662c9047",
   "metadata": {},
   "source": [
    "### File access\n",
    "\n",
    "The entire dataset is made up of several dozen individual netCDF files, each representing an entire global inundation map, but derived from either a diferent source DEM, sea level rise condition, or return period. Return periods are occurence probabilities for floods of a particular magnitude, often referred to as, for example, \"a 100 year flood\".\n",
    "\n",
    "To start, we'll load and plot the inundation data produced from the 90m NASADEM/MERITDEM at a 0/2/5/10/25/100/250 year return period for 2050 sea level rise conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded932ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the set of parameters to iterate over\n",
    "\n",
    "dem_source_list = [\"NASADEM\", \"MERITDEM\"]\n",
    "return_period_list = [\"0000\", \"0002\", \"0005\", \"0010\", \"0025\", \"0100\", \"0250\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59efea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now import the reference grid shapefile\n",
    "\n",
    "grid = gpd.read_file(\"grid_reference_500.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97038d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we convert it to the same geographic format at the input flood map\n",
    "grid_coord = grid.to_crs(4326)\n",
    "\n",
    "# We also add the area of a pixel and clean the resulting data frame\n",
    "grid_coord['pixel_area'] = grid_coord.geometry.area\n",
    "grid_coord = grid_coord.loc[:, ['ID', 'geometry', 'pixel_area']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5174bdc",
   "metadata": {},
   "source": [
    "### Geographic Treatment\n",
    "\n",
    "We first adapt the imported data to the desired flood map format, as with CoCT's grid.\n",
    "Then, we associate to each grid pixel its share of flood-prone area and a flood depth level associated with the relative share of each flood points in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bad0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(year, dem_source, return_period):\n",
    "    root = (\n",
    "        \"https://deltaresfloodssa.blob.core.windows.net/floods/v2021.06\"\n",
    "    )\n",
    "    path = f\"{root}/global/{dem_source}/90m\"\n",
    "    file_name = f\"GFM_global_{dem_source}90m_{year}slr_rp{return_period}_masked.nc\"\n",
    "\n",
    "    return f\"{path}/{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_treatment(dem_source, return_period, flooded, grid_coord, year):\n",
    "\n",
    "    # We keep only relevant data from source\n",
    "\n",
    "    flooded_inun = flooded['inun']\n",
    "    da = flooded_inun.to_dataframe().reset_index()\n",
    "\n",
    "    # We convert associated points to a GeoDataFrame for geographic treatment\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        da.inun, geometry=gpd.points_from_xy(da.lon,da.lat), crs=\"EPSG:4326\")\n",
    "\n",
    "    # We clean the output\n",
    "\n",
    "    gdf = gdf.loc[~np.isnan(gdf['inun'])]\n",
    "\n",
    "    # We create a square buffer of side 90m (~0.03*(1/60) degrees in our geometry) around our centroids\n",
    "    # This corresponds to the resolution of the flood map\n",
    "    gdf_buffer = gdf.buffer(0.03*(1/60), cap_style=3)\n",
    "\n",
    "    # We plot the output for validation\n",
    "    fig, ax1 = plt.subplots()\n",
    "    gdf_buffer.boundary.plot(ax=ax1, color = 'slategrey')\n",
    "    gdf.plot(ax = ax1, color = 'red')\n",
    "\n",
    "    # We update the geometry of out GeoDataFrame with the buffer polygons\n",
    "\n",
    "    gdf_new = gdf\n",
    "    gdf_new['geometry'] = gdf_buffer\n",
    "\n",
    "    # We save the associated shapefile\n",
    "\n",
    "    gdf_new.to_file(dem_source + '_' + return_period + '.shp')\n",
    "\n",
    "    # We get the intersection of the two data frames and get the associated area\n",
    "    grid_overlay = gpd.overlay(gdf_new, grid_coord)\n",
    "    grid_overlay['inter_area'] = grid_overlay.geometry.area\n",
    "\n",
    "    # We plot the output for validation\n",
    "    fig, ax1 = plt.subplots()\n",
    "    grid_coord.boundary.plot(ax = ax1, color = 'slategrey')\n",
    "    grid_overlay.boundary.plot(ax=ax1, color = 'red')\n",
    "\n",
    "    # Note that the grid perimeter is a bit wider than CoCT's territorial extent, hence some coastal\n",
    "    # points appear to be inland: this is not a problem as sea points have zero land availability\n",
    "    # in the model\n",
    "\n",
    "    # We first compute the proportion of flood-prone area in each pixel\n",
    "    sum_flood_area = grid_overlay.groupby('ID')['inter_area'].agg('sum')\n",
    "    pixel_area = grid_overlay.groupby('ID')['pixel_area'].agg('mean')\n",
    "    prop_flood_prone = pd.Series(sum_flood_area / pixel_area, name='prop_flood_prone')\n",
    "    # The following correction only applies to two pixels (out of 401)\n",
    "    prop_flood_prone[prop_flood_prone > 1] = 1\n",
    "\n",
    "    # We then get the flood depth per pixel as a weighted average of flood depth levels in each\n",
    "    # pixel intersections with original flood data\n",
    "    grid_overlay['w_inun'] = grid_overlay['inun'] * grid_overlay['inter_area']\n",
    "    sum_w_inun = grid_overlay.groupby('ID')['w_inun'].agg('sum')\n",
    "    flood_depth = pd.Series(sum_w_inun / sum_flood_area, name='flood_depth')\n",
    "\n",
    "    output = pd.merge(flood_depth, prop_flood_prone, on='ID')\n",
    "    print(output)\n",
    "\n",
    "    # We merge back the output with CoCT's grid to cover the full extent of the city\n",
    "    # (not only the flooded pixels)\n",
    "    \n",
    "    slr = \"\"\n",
    "    if year == 2050:\n",
    "        slr = \"1\"\n",
    "    elif year == 2018:\n",
    "        slr = \"0\"\n",
    "    \n",
    "    result = pd.merge(grid_coord, output, left_on='ID', right_index=True, how='outer')\n",
    "    result.to_file(\"C_\" + dem_source + \"_\" + slr + \"_\" + return_period + \".shp\")\n",
    "\n",
    "    # We also export the results to the same format as for FATHOM data\n",
    "\n",
    "    result_export = result.loc[:,['flood_depth', 'prop_flood_prone']]\n",
    "    result_export.flood_depth[np.isnan(result_export['flood_depth'])] = 0\n",
    "    result_export.prop_flood_prone[np.isnan(result_export['prop_flood_prone'])] = 0\n",
    "    result_export.to_excel(\"C_\" + dem_source + \"_\" + slr + \"_\" + return_period + \".xlsx\", index=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5847b",
   "metadata": {},
   "source": [
    "### Loop over parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i = 0\n",
    "\n",
    "for year in [2018, 2050]:\n",
    "    for dem_source in dem_source_list:\n",
    "        for return_period in return_period_list:\n",
    "        \n",
    "            with fsspec.open(make_url(year, dem_source, return_period)) as f:\n",
    "                ds = xr.open_dataset(f, chunks={\"lat\": 5000, \"lon\": 5000})\n",
    "                ds_coct = ds.sel(lat=slice(miny, maxy), lon=slice(minx, maxx))\n",
    "                # Select only flooded area\n",
    "                flooded = ds_coct.where(ds_coct.inun > 0).isel(time=0).compute()\n",
    "            result = geo_treatment(dem_source, return_period, flooded, grid_coord)\n",
    "            results.append(result)\n",
    "\n",
    "            i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
